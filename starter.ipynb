{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_identity = pd.read_csv(f'{folder_path}train_identity.csv')\n",
    "train_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n",
    "test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n",
    "test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\n",
    "sub = pd.read_csv(f'{folder_path}sample_submission.csv')\n",
    "# let's combine the data and work with the whole dataset\n",
    "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n",
    "\n",
    "\n",
    "train['user'] = train['card1'].astype(str) + roman_df['subcard_categorical'][:N_TRAIN_EXAMPLES].astype(str)\n",
    "test['user'] = test['card1'].astype(str) + roman_df['subcard_categorical'][N_TRAIN_EXAMPLES:].astype(str)\n",
    "inter_cards = set(train['user']).intersection(set(test['user']))\n",
    "mean_fraud = train[train['user'].isin(inter_cards)].groupby('user')['isFraud'].agg(['mean', 'count'])\n",
    "max_fraud = mean_fraud[mean_fraud['mean'] == 1]\n",
    "mask = test['user'].isin(max_fraud.index)\n",
    "top_sub[mask]['isFraud'] = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_cols = !head -1 data/train_identity.csv\n",
    "identity_cols = identity_cols[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "folder_path = 'data/'\n",
    "train = pd.read_pickle(f'{folder_path}train.pkl')\n",
    "test = pd.read_pickle(f'{folder_path}test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_EXAMPLES = 590540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_EXAMPLES = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_am = pd.read_csv(f'{folder_path}train_transaction.csv', usecols=['TransactionAmt'], dtype=str)\n",
    "te_am = pd.read_csv(f'{folder_path}test_transaction.csv', usecols=['TransactionAmt'], dtype=str)\n",
    "train[['dollars', 'cents']] = tr_am.TransactionAmt.str.split('.', expand=True).astype(int)\n",
    "test[['dollars', 'cents']] = te_am.TransactionAmt.str.split('.', expand=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_df = pd.read_pickle('all_new_ids_roman_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roman_feature_names = set(roman_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES = set(test.columns) - set(['TransactionDT', 'TransactionID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = set(test.columns) - set(['TransactionDT', 'TransactionID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES =  set(['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19',\n",
    "            'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n",
    "            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37',\n",
    "            'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n",
    "            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', \n",
    "            'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Говорят, что неплохо бы дропнуть те карты, которые не встречается или в трейне, или в тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['card1']: \n",
    "\n",
    "    print('No intersection in Train', len(train[~train[col].isin(test[col])]))\n",
    "    print('Intersection in Train', len(train[train[col].isin(test[col])]))\n",
    "    \n",
    "    train[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n",
    "    test[col]  = np.where(test[col].isin(train[col]), test[col], np.nan)\n",
    "    print('#'*20)\n",
    "\n",
    "for col in ['card2','card3','card4','card5','card6',]: \n",
    "    print('No intersection in Train', col, len(train[~train[col].isin(test[col])]))\n",
    "    print('Intersection in Train', col, len(train[train[col].isin(test[col])]))\n",
    "    \n",
    "    train[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n",
    "    test[col]  = np.where(test[col].isin(train[col]), test[col], np.nan)\n",
    "    print('#'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train, test])\n",
    "all_df.reset_index(inplace=True, drop=True)\n",
    "all_df = pd.concat([all_df, roman_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фичи по датам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "all_df['DT'] = all_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test, roman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_features(all_df):\n",
    "    all_df['DT'] = all_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    all_df['dayofweek'] = all_df['DT'].dt.dayofweek\n",
    "    all_df['dayofmonth'] = all_df['DT'].dt.day\n",
    "    all_df['hour'] = all_df['DT'].dt.hour\n",
    "    all_df['weekofmonth'] = (all_df['DT'].dt.day - 1) // 7 + 1\n",
    "    new_features = ['dayofweek', 'dayofmonth', 'hour', 'weekofmonth']\n",
    "    return new_features, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a, c = datetime_features(all_df)\n",
    "MODEL_FEATURES.update(a)\n",
    "CATEGORICAL_FEATURES.update(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['DT_split'] = (all_df['DT'].dt.year - 2017) * 12 + all_df['DT'].dt.month\n",
    "N_TRAIN = sum(all_df['DT_split'] < 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['day'] = ((all_df['DT'].dt.year-2017)*365 + all_df['DT'].dt.dayofyear).astype(np.int16)\n",
    "all_df['hour_day'] = all_df['DT'].dt.hour + 24 * (all_df['day'] - 335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols = [col for col in all_df.columns if col.startswith('D') and not col[-1:].isalpha() and col != 'D9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols_notnull = [d + '_notnull' for d in d_cols + ['D9']]\n",
    "all_df[d_cols_notnull] = all_df[d_cols + ['D9']].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets transform D8 and D9 column\n",
    "# As we almost sure it has connection with hours\n",
    "all_df['D8_not_same_day'] = np.where(all_df['D8']>=1,1,0)\n",
    "all_df['D8_D9_decimal_dist'] = all_df['D8'].fillna(0)-all_df['D8'].fillna(0).astype(int)\n",
    "all_df['D8_D9_decimal_dist'] = ((all_df['D8_D9_decimal_dist']-all_df['D9'])**2)**0.5\n",
    "all_df['D8'] = all_df['D8'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(['D8_not_same_day', 'D8_D9_decimal_dist', 'D8_D9_decimal_dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(all_df, period, col, clip=True, minmax=True):\n",
    "        new_col = col + '_' + period\n",
    "        df = all_df[[col, period]].copy()\n",
    "        df[col] = df[col].astype(float)\n",
    "        if clip:\n",
    "            df[col] = df[col].clip(0) \n",
    "\n",
    "        aggs = df.groupby([period])[col].agg(['min', 'max', 'std', 'mean'])\n",
    "        \n",
    "        agg_max = aggs['max'].to_dict()\n",
    "        agg_min = aggs['min'].to_dict()\n",
    "        agg_std = aggs['std'].to_dict()\n",
    "        agg_mean = aggs['mean'].to_dict()\n",
    "\n",
    "        all_df['temp_min'] = all_df[period].map(agg_max)\n",
    "        all_df['temp_max'] = all_df[period].map(agg_min)\n",
    "        all_df['temp_std'] = all_df[period].map(agg_std)\n",
    "        all_df['temp_mean'] = all_df[period].map(agg_mean)\n",
    "        \n",
    "\n",
    "        all_df[new_col + '_min_max'] = ((all_df[col] - all_df['temp_min']) /\\\n",
    "                (all_df['temp_max'] - all_df['temp_min'])).astype(float)\n",
    "        \n",
    "        all_df[new_col + '_std_score'] = (all_df[col] - all_df['temp_mean']) / (all_df['temp_std'])\n",
    "\n",
    "        del all_df['temp_min'], all_df['temp_max'], all_df['temp_std'], all_df['temp_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for period in ['day']:\n",
    "    for col in d_cols:\n",
    "        values_normalization(all_df, period, col, minmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['D1','D2']:\n",
    "    all_df[col + '_scaled'] = all_df[col] / all_df[:N_TRAIN_EXAMPLES][col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(['D1_scaled', 'D2_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['DeviceInfo'].fillna('', inplace=True)\n",
    "all_df['id_30'].fillna('', inplace=True)\n",
    "all_df['id_31'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_device_features(all_df):\n",
    "    all_df['DeviceInfoMajor'] = all_df['DeviceInfo'].str.split(' ', expand=True)[0]\n",
    "    all_df['DeviceInfoTop'] = all_df['DeviceInfo'].str.split('-', expand=True)[0]\n",
    "    all_df['DeviceInfoIsRV'] = all_df['DeviceInfoMajor'].apply(lambda x: 'rv' in x)\n",
    "    return ['DeviceInfoMajor', 'DeviceInfoTop', 'DeviceInfoIsRV'], ['DeviceInfoMajor', 'DeviceInfoTop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a, c = add_device_features(all_df)\n",
    "MODEL_FEATURES.update(a)\n",
    "CATEGORICAL_FEATURES.update(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_version(version):\n",
    "    n_sep_a = version.count('.')\n",
    "    n_sep_b = version.count('_')\n",
    "    if n_sep_a > 0:\n",
    "        return version.split('.')[0]\n",
    "    elif n_sep_b > 0:\n",
    "        return version.split('_')[0]\n",
    "    else:\n",
    "        return version\n",
    "\n",
    "def split_os(os):\n",
    "    spl = os.split(' ')\n",
    "    if len(spl) > 1:\n",
    "        os_name = ' '.join(spl[:-1])\n",
    "        major_version = split_version(spl[-1])\n",
    "        return os_name, ' '.join([os_name, major_version])\n",
    "    else:\n",
    "        return os, ''\n",
    "# TODO: add minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_os_features(all_df):\n",
    "    os, version = zip(*all_df['id_30'].apply(lambda x: split_os(x)).values)\n",
    "    all_df['OSName'] = os\n",
    "    all_df['OSMajorVersion'] = version\n",
    "    return ['OSName', 'OSMajorVersion'], ['OSName', 'OSMajorVersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a, c = add_os_features(all_df)\n",
    "MODEL_FEATURES.update(a)\n",
    "CATEGORICAL_FEATURES.update(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAME ON ME\n",
    "def get_browser(browser):\n",
    "    if 'safari' in browser:\n",
    "        return 'safari'\n",
    "    \n",
    "    if 'chrome' in browser:\n",
    "        return 'chrome'\n",
    "    \n",
    "    if browser.startswith('ie'):\n",
    "        return 'internetexplorer'\n",
    "    \n",
    "    if 'edge' in browser:\n",
    "        return 'edge'\n",
    "    \n",
    "    if 'firefox' in browser.lower():\n",
    "        return 'firefox'\n",
    "    \n",
    "    if 'samsung' in browser.lower():\n",
    "        return 'samsung'\n",
    "    \n",
    "    if 'google' in browser:\n",
    "        return 'google'\n",
    "    \n",
    "    if 'opera' in browser:\n",
    "        return 'opera'\n",
    "    \n",
    "    if 'android' in browser.lower():\n",
    "        return 'android'\n",
    "    \n",
    "    return browser\n",
    "\n",
    "def is_mobile(browser):\n",
    "    br = browser.lower()\n",
    "    if 'mobile' in br or 'for android' in br:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_browser_features(all_df):\n",
    "    all_df['Browser'] = all_df['id_31'].apply(lambda x: get_browser(x))\n",
    "    all_df['IsMobile'] = all_df['id_31'].apply(lambda x: is_mobile(x))\n",
    "    #all_df['VersionNum'] = all_df['id_31'].fillna('0').\\\n",
    "    #    apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    #all_df['BrowserVersion'] = all_df['Browser'] + ' ' + all_df['VersionNum'].astype('str')\n",
    "    \n",
    "    return ['Browser', 'IsMobile'],  ['Browser', 'IsMobile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a, c = get_browser_features(all_df)\n",
    "MODEL_FEATURES.update(a)\n",
    "CATEGORICAL_FEATURES.update(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some with M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### M columns (except M4)\n",
    "# All these columns are binary encoded 1/0\n",
    "# We can have some features from it\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "all_df['M_sum'] = all_df[i_cols].sum(axis=1).astype(np.int8)\n",
    "all_df['M_na'] = all_df[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "MODEL_FEATURES.update(['M_sum', 'M_na'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['notnull_count'] = all_df[basic_features].notnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.add('notnull_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_uid = ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[cols_for_uid] = all_df[cols_for_uid].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_registration_date(df):\n",
    "    tr_dt = 'TransactionDT'\n",
    "\n",
    "    df[f'{tr_dt}_to_datetime'] = df[tr_dt].apply(\n",
    "        lambda x: START_DATE + datetime.timedelta(seconds=x)\n",
    "    )\n",
    "\n",
    "    df['card_registered_delta_tmp'] = pd.to_timedelta(df['D1'], unit='day')\n",
    "    df['subcard_reg_date'] = (\n",
    "            df['TransactionDT_to_datetime'] - df['card_registered_delta_tmp']\n",
    "    )\n",
    "    df['subcard_reg_timestamp'] = df['subcard_reg_date']\\\n",
    "        .dt\\\n",
    "        .date\\\n",
    "        .apply(\n",
    "        lambda x: (\n",
    "                x - datetime.date(1970, 1, 1)\n",
    "        ).total_seconds()\n",
    "    )\n",
    "    df['subcard_categorical'] = df['subcard_reg_date']\\\n",
    "        .dt\\\n",
    "        .date\\\n",
    "        .astype(str)\n",
    "\n",
    "    df.drop(\n",
    "        labels=[\n",
    "            'card_registered_delta_tmp',\n",
    "            'subcard_reg_date'\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = extract_registration_date(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['subcard_categorical_full'] = all_df['card1'] + '_' + all_df['subcard_categorical'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_add = [('id_19', 'DeviceType', 'id_15'),\n",
    "            ('id_19', 'id_15', 'R_emaildomain'),\n",
    "            ('id_15', 'id_38', 'id_13')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcard_id_cols = ['addr1', 'DeviceInfo', 'P_emaildomain', 'ProductCD', 'id_20', 'id_19', 'card4']\n",
    "subcard_id_cols.extend(also_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcard_ids = []\n",
    "for coll in subcard_id_cols:\n",
    "    if not isinstance(coll, str):\n",
    "        col = '_'.join(coll)\n",
    "        all_df['subcard_' + col] = all_df['subcard_categorical_full'].astype(str) + '_' +\\\n",
    "            all_df[list(coll)].astype(str).apply(lambda x: '_'.join(x), axis=1)\n",
    "    else:\n",
    "        col = coll\n",
    "        all_df['subcard_' + col] = all_df['subcard_categorical_full'].astype(str) + '_' +\\\n",
    "            all_df[coll].astype(str)\n",
    "    \n",
    "    subcard_ids.append('subcard_' + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(['subcard_categorical', 'subcard_categorical_full'])\n",
    "CATEGORICAL_FEATURES.update(['subcard_categorical', 'subcard_categorical_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(subcard_ids)\n",
    "CATEGORICAL_FEATURES.update(subcard_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['uid1'] = all_df['card1'] + '_' + all_df['card2']\n",
    "\n",
    "all_df['uid2'] = all_df['uid1'] + '_' + all_df['card3'] + '_' + all_df['card5']\n",
    "\n",
    "all_df['uid3'] = all_df['uid2'] + '_' + all_df['addr1'] + '_' + all_df['addr2']\n",
    "\n",
    "all_df['uid4'] = all_df['uid3'] + '_' + all_df['P_emaildomain']\n",
    "\n",
    "all_df['uid5'] = all_df['uid3'] + '_' + all_df['R_emaildomain']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = [f'uid{i}' for i in range(1, 6)] + ['subcard_categorical_full'] + subcard_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcard_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TransactionAmt features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Говорят, если в сумме транзакции есть более двух чисел после запятой, то это транзакция в иностранной валюте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['is_foreign'] = all_df['cents'].apply(lambda x: len(str(x)) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.add('is_foreign')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А раз уникальных TransactionAmt не очень много, то почему бы не добавить nunique для каждого из айдишников сверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unique_amt_cols = []\n",
    "for uid in uids:\n",
    "    uniques = all_df.groupby(uid)['TransactionAmt'].nunique()\n",
    "    all_df[uid + '_unique_amt'] = all_df[uid].map(uniques)\n",
    "    unique_amt_cols.append(uid + '_unique_amt')\n",
    "    \n",
    "QT = QuantileTransformer(n_quantiles=500)\n",
    "all_df[unique_amt_cols] = QT.fit_transform(all_df[unique_amt_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(unique_amt_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Следующая транзакция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "near_cols = []\n",
    "same_cols = []\n",
    "for col in ['card1', 'subcard_categorical_full'] + subcard_ids:\n",
    "    print(col)\n",
    "    for i in range(1, 5):\n",
    "        print(i)\n",
    "        a = 'is_same_next_transaction_' + str(i)\n",
    "        b = 'is_same_prev_transaction_' + str(i)\n",
    "        all_df[a] = all_df.groupby(col)['TransactionAmt'].diff(i) == 0\n",
    "        all_df[b] = all_df.groupby(col)['TransactionAmt'].diff(-i) == 0\n",
    "        same_cols.extend([a, b])\n",
    "    all_df[f'same_transaction_near_{col}'] = all_df[same_cols].sum(axis=1)\n",
    "    all_df.drop(same_cols, axis=1, inplace=True)\n",
    "    near_cols.append(f'same_transaction_near_{col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(near_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_frequency(col, quantile=True):\n",
    "    return col.map(col.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "freq_cols = []\n",
    "for col in list(CATEGORICAL_FEATURES) + uids + ['cents', 'dollars'] :\n",
    "    all_df[col + '_freq'] = encode_frequency(all_df[col])\n",
    "    freq_cols.append(col + '_freq')\n",
    "    \n",
    "QT = QuantileTransformer(n_quantiles=500)\n",
    "all_df[freq_cols] = QT.fit_transform(all_df[freq_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(freq_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(col):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_df[col].astype(str).values)\n",
    "    return le.transform(all_df[col].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['cents_categorical'] = all_df['cents'].copy()\n",
    "all_df['dollars_categorical'] = all_df['dollars'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES.update(['cents_categorical', 'dollars_categorical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(['cents_categorical', 'dollars_categorical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with Pool(16) as pool:\n",
    "    encodes = pool.map(encode, CATEGORICAL_FEATURES)\n",
    "\n",
    "for name, enc in zip(CATEGORICAL_FEATURES, encodes):\n",
    "    all_df[name] = enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smooth_encoding(all_df, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = all_df[on].mean()\n",
    "    \n",
    "    std = all_df[on].std()\n",
    "    \n",
    "    median = np.nanmedian(all_df[on])\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = all_df.groupby(by)[on].agg(['count', 'mean', 'std', np.nanmedian])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "    stds = agg['std']\n",
    "    medians = agg['nanmedian']\n",
    "    \n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "    \n",
    "    smooth_std = (stds * counts + m * std ) / (counts + m)\n",
    "    \n",
    "    smooth_median = (medians * counts + m * median ) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    return all_df[by].map(smooth), all_df[by].map(smooth_std), all_df[by].map(smooth_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "amt_features = []\n",
    "for col in uids:\n",
    "    mean, std, median = calc_smooth_encoding(all_df, col, 'TransactionAmt', 30)\n",
    "    all_df[col + '_TransactionAmt_mean'] = mean\n",
    "    all_df[col + '_TransactionAmt_std'] = std\n",
    "    #all_df[col + '_TransactionAmt_median'] = median\n",
    "    amt_features.extend([col + '_TransactionAmt_mean', col + '_TransactionAmt_std'])#, col + '_TransactionAmt_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D15_features = []\n",
    "for col in uids:\n",
    "    mean, std, median = calc_smooth_encoding(all_df, col, 'D15_day_min_max', 30)\n",
    "    all_df[col + '_D15_mean'] = mean\n",
    "    all_df[col + '_D15_std'] = std\n",
    "    #all_df[col + '_D15_median'] = median\n",
    "    D15_features.extend([col + '_D15_mean', col + '_D15_std'])#, col + '_D15_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "C13_features = []\n",
    "for col in uids:\n",
    "    mean, std, median = calc_smooth_encoding(all_df, col, 'C13', 30)\n",
    "    all_df[col + '_C13_mean'] = mean\n",
    "    all_df[col + '_C13_std'] = std\n",
    "    #all_df[col + '_C13_median'] = median\n",
    "    C13_features.extend([col + '_C13_mean', col + '_C13_std'])#, col + '_C13_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(D15_features + C13_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(amt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_normalization(all_df, 'day', 'subcard_reg_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.add('subcard_reg_timestamp_day_min_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle('many_ids_all_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "def deskew_this_data(df, col_names, min_skew, method='yeo-johnson'):\n",
    "\n",
    "    transform_cols = []\n",
    "    for col in col_names:\n",
    "        if skew(df[col]) > min_skew:\n",
    "            transform_cols.append(col)\n",
    "\n",
    "    X = df[transform_cols]\n",
    "    n = len(transform_cols)\n",
    "    if n == 0:\n",
    "        return\n",
    "    elif n == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df[transform_cols] = power_transform(X, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(MODEL_FEATURES - CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['uid1_TransactionAmt_std'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = power_transform(all_df['uid1_TransactionAmt_std'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(MODEL_FEATURES - CATEGORICAL_FEATURES)\n",
    "mask = all_df[numeric_features].dtypes != 'O'\n",
    "numeric_final = np.array(numeric_features)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = power_transform(all_df[numeric_final], method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(all_df['uid1_TransactionAmt_std'], nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_normalization(all_df, 'day', 'subcard_reg_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = all_df[:N_TRAIN_EXAMPLES]\n",
    "test_final = all_df[N_TRAIN_EXAMPLES:]\n",
    "\n",
    "tr = train_final[:N_TRAIN]\n",
    "val = train_final[N_TRAIN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, how_strong=2):\n",
    "    positive = df[df.isFraud == 1]\n",
    "    negative = df[df.isFraud == 0]\n",
    "    negative = negative.sample(int(len(negative) / how_strong))\n",
    "    res = pd.concat([positive, negative])\n",
    "    return res.sample(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = downsample(tr, 10)\n",
    "val = downsample(val, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cols = [col + '_day_min_max' for col in d_cols] \n",
    "add_cols = add_cols + [col + '_day_std_score' for col in d_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'old_roman/IEEE_FRAUD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import CATEGORICAL_FEATURES as roman_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_categorical = list(set(roman_categorical) - set(['is_holiday']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = set(list(MODEL_FEATURES) + list(roman_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols = [col for col in all_df.columns if col.startswith('D') \n",
    "          and not col[-1:].isalpha() and col != 'D9' and len(col) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols = list(MODEL_FEATURES - set(['card1']) - set(d_cols) ) + add_cols + ['subcard_reg_timestamp_day_min_max']\n",
    "good_categotical = list(set(CATEGORICAL_FEATURES) - set(['card1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_good_features = set(good_cols\n",
    "                        + list(roman_feature_names)) -\\\n",
    "    set(['card1', 'dayofmonth'])\n",
    "all_good_categorical = set(good_categotical + roman_categorical) - set(['card1', 'is_holiday', 'dayofmonth'])\n",
    "\n",
    "all_good_categorical = all_good_categorical.intersection(all_good_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('device_info_col_names.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_FEATURES, CATEGORICAL_FEATURES, roman_feature_names, roman_categorical,\n",
    "                all_good_features, all_good_categorical), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_good_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ids, val_ids in gkf.split(train_final, train_final['isFraud'], train_final['DT_split']):\n",
    "    df_tr = train_final.loc[train_ids]\n",
    "    df_val = train_final.loc[val_ids]\n",
    "    dtrain = lgb.Dataset(df_tr[all_good_features], label=df_tr['isFraud'],\n",
    "                         categorical_feature=all_good_categorical, free_raw_data=False)\n",
    "    dval = lgb.Dataset(df_val[all_good_features], label=df_val['isFraud'],\n",
    "                       categorical_feature=all_good_categorical, free_raw_data=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(tr[all_good_features], label=tr['isFraud'],\n",
    "                     categorical_feature=all_good_categorical,free_raw_data=False)\n",
    "dval = lgb.Dataset(val[all_good_features], label=val['isFraud'],\n",
    "                   categorical_feature=all_good_categorical, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_features(drop_feature):\n",
    "    features = list(set(all_good_features) - set([drop_feature]))\n",
    "    categorical = list(set(all_good_categorical) - set([drop_feature]))\n",
    "    dtrain = lgb.Dataset(tr[features], label=tr['isFraud'],\n",
    "                     categorical_feature=categorical,\n",
    "                         free_raw_data=False)\n",
    "    dval = lgb.Dataset(val[features], label=val['isFraud'],\n",
    "                   categorical_feature=categorical,\n",
    "                       free_raw_data=False)\n",
    "    \n",
    "    model = lgb.train(params, dtrain, num_boost_round=10000,\n",
    "                  valid_sets=(dval, dtrain), valid_names=('val', 'train'),\n",
    "                early_stopping_rounds=25, verbose_eval=0)\n",
    "    return model.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 200,\n",
    "          'min_child_samples': 40,\n",
    "          #'min_sum_hessian_in_leaf': 5e-3,\n",
    "          #'max_bin': 1023,\n",
    "          #'min_data_in_leaf': \n",
    "          #'scale_pos_weight': 2,\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'max_depth': 13,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"goss\",\n",
    "          \"top_rate\": 0.6,\n",
    "          \"other_rate\": 0.1,\n",
    "          \"bagging_freq\": 0,\n",
    "          'nthread': 16,\n",
    "          \n",
    "          'pos_bagging_fraction': 0.8,\n",
    "          'neg_bagging_fraction': 0.01,\n",
    "          \"bagging_fraction\": 0.6,\n",
    "          \"bagging_seed\": 11,\n",
    "          'reg_alpha': 0.3,\n",
    "          'reg_lambda': 0.3,\n",
    "          \n",
    "          'feature_fraction': 0.7,\n",
    "          'min_data_per_group': 25,\n",
    "          'cat_smooth': 500,\n",
    "          'max_cat_to_onehot': 8\n",
    "          #'categorical_feature': cat_cols\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = lgb.train(params, dtrain, num_boost_round=10000,\n",
    "                  valid_sets=(dval, dtrain), valid_names=('val', 'train'),\n",
    "                early_stopping_rounds=25, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.928021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.94599 - дропаем -1e-4\n",
    "0.946581 - дропаем -1e-3\n",
    "0.949..- ничего не дропаем\n",
    "0.949693 - дропнул 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_AUC = 0.9451639985019628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    feature_metric_loss = {}\n",
    "    for i, feature in enumerate(all_good_features):\n",
    "        best_score = fit_on_features(feature)\n",
    "        feature_metric_loss[feature] = DEFAULT_AUC - best_score['val']['auc']\n",
    "        print(feature, f'with №{i} loss change:', feature_metric_loss[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature_auc_loss.json', 'w') as f:\n",
    "    json.dump(feature_metric_loss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame.from_dict(feature_metric_loss, orient='index')\n",
    "feature_imp.reset_index(inplace=True)\n",
    "feature_imp.columns = ['Feature', 'Value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp[feature_imp['Value'] < -0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[-100:])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = lgb.train(params, dtrain, num_boost_round=10000,\n",
    "                  valid_sets=(dval, dtrain), valid_names=('val', 'train'),\n",
    "                early_stopping_rounds=25, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.945351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.947028 0.946727 0.946764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = lgb.train(params, dtrain, num_boost_round=10000,\n",
    "                  valid_sets=(dval, dtrain), valid_names=('val', 'train'),\n",
    "                early_stopping_rounds=25, verbose_eval=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importance('split'), good_cols)), columns=['Value','Feature'])\n",
    "bad_cols = feature_imp[feature_imp['Value'] < 1]['Feature']\n",
    "len(bad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importance('split'), all_good_features)), columns=['Value','Feature'])\n",
    "bad_cols = feature_imp[feature_imp['Value'] < 1]['Feature']\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:100])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_test = test_final.sample(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = model.predict(shap_test[all_good_features], pred_contrib=True)\n",
    "mean_shap = np.abs(res).mean(axis=0)\n",
    "shap_imp = pd.DataFrame(sorted(zip(mean_shap, all_good_features)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=shap_imp.sort_values(by=\"Value\", ascending=False)[:60])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit KFold (Scary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 200,\n",
    "          'min_child_samples': 40,\n",
    "          #'min_sum_hessian_in_leaf': 5e-3,\n",
    "          #'max_bin': 1023,\n",
    "          #'min_data_in_leaf': \n",
    "          #'scale_pos_weight': 2,\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'max_depth': 13,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"top_rate\": 0.6,\n",
    "          \"other_rate\": 0.1,\n",
    "          \"bagging_freq\": 0,\n",
    "          'nthread': 16,\n",
    "          \n",
    "          'pos_bagging_fraction': 0.8,\n",
    "          'neg_bagging_fraction': 0.01,\n",
    "          \"bagging_fraction\": 0.7,\n",
    "          \"bagging_seed\": 11,\n",
    "          'reg_alpha': 0.3,\n",
    "          'reg_lambda': 0.3,\n",
    "          \n",
    "          'feature_fraction': 0.9,\n",
    "          'min_data_per_group': 25,\n",
    "          'cat_smooth': 500,\n",
    "          'max_cat_to_onehot': 8\n",
    "          #'categorical_feature': cat_cols\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtrain, dval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_good_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "for train_ids, val_ids in gkf.split(train_final, train_final['isFraud'], train_final['DT_split']):\n",
    "    df_tr = train_final.loc[train_ids]\n",
    "    df_val = train_final.loc[val_ids]\n",
    "    dtrain = lgb.Dataset(df_tr[all_good_features], label=df_tr['isFraud'],\n",
    "                         categorical_feature=all_good_categorical, free_raw_data=False)\n",
    "    dval = lgb.Dataset(df_val[all_good_features], label=df_val['isFraud'],\n",
    "                       categorical_feature=all_good_categorical, free_raw_data=False)\n",
    "    \n",
    "    \n",
    "    model = lgb.train(params, dtrain, num_boost_round=10000, valid_sets=(dval),\n",
    "                early_stopping_rounds=100, verbose_eval=100)\n",
    "    preds = model.predict(test_final[all_good_features], num_iteration=model.best_iteration)\n",
    "    \n",
    "    all_preds.append(preds)\n",
    "    scores.append(model.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[score['valid_0']['auc'] for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = np.mean([score['valid_0']['auc'] for score in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "gmean_preds = gmean(all_preds, axis=0)\n",
    "np.corrcoef(gmean_preds, np.mean(all_preds, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'submissions/lgb_with_many_ids{best_score:0.7}.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "sub['isFraud'] = gmean_preds\n",
    "sub.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!KAGGLE_USERNAME=tishur KAGGLE_KEY=28da1297bec180204c1c524afa6f3d2e kaggle competitions submit ieee-fraud-detection -f {filename} -m \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'submissions/kfold_without_te_12_folds{best_score:0.4}.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.read_csv('submissions/gkfold_d_min_max_0.94122.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sub.isFraud, sub2.isFraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['is_test'] = [0 for _ in range(len(train))] + [1 for _ in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_df = all_df.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols = list(MODEL_FEATURES - set(['VersionNum'])) #- set(baaad))# -\\\n",
    "#                 set(['BrowserVersion', 'V9', 'id_31', 'id_13']))\n",
    "good_categotical = list(set(CATEGORICAL_FEATURES))# - set(baaad))# -\\\n",
    "#                        set([ 'BrowserVersion', 'id_31', 'id_13']))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_adv = lgb.Dataset(adv_df[good_cols], label=adv_df['is_test'], categorical_feature=good_categotical,\n",
    "                    free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model = lgb.train(params, d_adv,\n",
    "                      num_boost_round=60,\n",
    "                      verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = adv_model.predict(adv_df[good_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(adv_df['is_test'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['card1', 'card2', 'BrowserVersion', 'V9', 'id_31', 'id_13']\n",
    "baaad = ['id_13_freq', 'D15', 'dist1', 'D11', 'dayofmonth',\n",
    "         'D10', 'BrowserVersion_freq', 'id_31_freq',\n",
    "         'dayofmonth_freq', 'D4', 'C12', 'V326', 'id_38',\n",
    "         'M_na', 'V335', 'id_38_freq', 'V8', 'M9_freq', 'id_01', 'id_34', 'Browser',\n",
    "         'id_34_freq', 'OSMajorVersion', 'OSMajorVersion_freq','card5', 'id_32', 'id_30_freq', 'id_33_freq'] +\\\n",
    "        [f for f in all_df.columns if f.startswith('D') and len(f) <= 3] +\\\n",
    "        [f for f in all_df.columns if f.startswith('V') and len(f) <= 4] +\\\n",
    "        [f for f in all_df.columns if f.startswith('C') and len(f) <= 4] +\\\n",
    "        [f for f in all_df.columns if f.startswith('id_')\n",
    "         or f.startswith('uid') or f.startswith('addr')] +\\\n",
    "        [f for f in all_df.columns if f.startswith('M') and (len(f) <= 4 or f.endswith('freq'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_32'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['id_32'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[:len(train)]['V8'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[len(train):]['V8'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['BrowserVersion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample = adv_df.sample(5000)\n",
    "res = adv_model.predict(sample[good_cols], pred_contrib=True)\n",
    "mean_shap = np.abs(res).mean(axis=0)\n",
    "shap_imp = pd.DataFrame(sorted(zip(mean_shap, good_cols)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=shap_imp.sort_values(by=\"Value\", ascending=False)[:60])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_clf.fit(tr[good_cols].fillna(-1), tr['isFraud'], feature_name=good_cols, categorical_feature=good_categotical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = PermutationImportance(lgb_clf, 'roc_auc', refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = imp.fit(val[good_cols].fillna(-1), val['isFraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_cols = [col for col in all_df.columns if col.startswith('C') and len(col) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_pca = PCA(n_components=0.99).fit_transform(all_df[C_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C_pca[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cols = []\n",
    "for i, col in enumerate([f'C_PCA_{c}' for c in range(C_pca.shape[1])]):\n",
    "    all_df[col] = C_pca[:, i]\n",
    "    pca_cols.append(col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(pca_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_cols = [col for col in all_df.columns if col.startswith('V') and len(col) < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "V_pca = PCA(n_components=0.99).fit_transform(all_df[V_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pca_cols = []\n",
    "for i, col in enumerate([f'V_PCA_{c}' for c in range(V_pca.shape[1])]):\n",
    "    all_df[col] = V_pca[:, i]\n",
    "    v_pca_cols.append(col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES.update(v_pca_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = []\n",
    "d_cols = [col for col in all_df.columns if col.startswith('D') and len(col) < 3 and col != 'DT']\n",
    "for col in d_cols:\n",
    "    all_df[col + '_diff'] = all_df[col].diff()\n",
    "    diff_cols.append(col + '_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ['id_33_freq', 'V143', 'id_11', 'id_20_freq', 'V123', 'V323', 'V267', 'uid5_D15_std', 'V269', 'id_24', 'V212', 'V234', 'uid2_D15_std', 'V24', 'V259', 'V316', 'V12', 'V334', 'V115', 'V261', 'V77', 'M6', 'C8', 'V57', 'uid3_D15_std', 'V92', 'id_28', 'V67', 'V34', 'R_emaildomain_2', 'V228', 'C12', 'V4', 'V328', 'V248', 'DeviceType_freq', 'C6', 'V294', 'V106', 'V265', 'id_12_freq', 'V139', 'V250', 'V258', 'V39', 'V18', 'V32', 'V48', 'id_08', 'addr1', 'V80', 'id_02', 'V320', 'M6_freq', 'id_23', 'uid5_TransactionAmt_std', 'OSMajorVersion_freq', 'V111', 'V10', 'P_emaildomain_1_freq', 'V225', 'R_emaildomain_3_freq', 'V199', 'D9', 'V244', 'M7', 'V50', 'OSMajorVersion', 'id_12', 'id_37_freq', 'V313', 'V101', 'id_38', 'V231', 'V216', 'V186', 'V220', 'V293', 'V55', 'uid3_freq', 'V170', 'V84', 'V98', 'V140', 'V256', 'DeviceInfoMajor', 'id_32_freq', 'V181', 'id_25_freq', 'V284', 'V29', 'V58', 'R_emaildomain_1', 'addr2_freq', 'V301', 'id_18', 'dayofmonth_freq', 'V63', 'V298', 'V289', 'V281', 'V255', 'M_na', 'V292', 'M3_freq', 'uid2_D15_mean', 'D12_day_min_max', 'V81', 'V300', 'dayofweek_freq', 'V270', 'uid4_TransactionAmt_std', 'V15', 'V175', 'id_32', 'id_19', 'V69', 'V178', 'V36', 'V260', 'V242', 'D14_day_min_max', 'V251', 'V20', 'DeviceInfo', 'V224', 'V86', 'id_37', 'id_35_freq', 'V156', 'V276', 'V303', 'addr1_freq', 'V253', 'V168', 'V335', 'V132', 'V79', 'V266', 'id_15_freq', 'V208', 'V108', 'V90', 'V185', 'V91', 'decimal_len', 'R_emaildomain_1_freq', 'P_emaildomain_2_freq', 'V133', 'V167', 'C7', 'id_10', 'V271', 'V213', 'V198', 'id_05', 'DeviceInfo_freq', 'V94', 'V126', 'M8_freq', 'V46', 'id_17', 'V155', 'V262', 'id_01', 'V53', 'V273', 'uid4_D15_mean', 'M4', 'V17', 'id_22', 'V286', 'V200', 'V246', 'id_17_freq', 'C10', 'M8', 'id_31_freq', 'V161', 'card5', 'V218', 'V26', 'V162', 'V338', 'V307', 'R_emaildomain', 'V5', 'decimal_value_freq', 'uid3_D15_mean', 'uid5_C13_mean', 'id_16_freq', 'V254', 'is_holiday_freq', 'V183', 'P_emaildomain_2', 'V76', 'P_emaildomain', 'V188', 'V195', 'V124', 'V75', 'dayofmonth', 'V95', 'D6_day_min_max', 'V35', 'V308', 'uid2_unique_amt', 'id_30', 'V336', 'V193', 'R_emaildomain_2_freq', 'V2', 'V127', 'uid4_freq', 'V45', 'V33', 'M1', 'uid2_TransactionAmt_std', 'weekofmonth', 'V73', 'id_28_freq', 'C1', 'V99', 'DeviceInfoTop', 'V243', 'V215', 'V104', 'uid4_C13_mean', 'card4_freq', 'id_34', 'V222', 'dist2', 'V83', 'M9', 'V49', 'OSName_freq', 'M3', 'V169', 'id_30_freq', 'V235', 'V311', 'D7_day_min_max', 'V205', 'V103', 'V209', 'V203', 'id_36', 'V149', 'D11_day_min_max', 'V290', 'V166', 'M9_freq', 'V236', 'V112', 'id_19_freq', 'V116', 'V31', 'id_14', 'V291', 'uid5_TransactionAmt_mean', 'id_38_freq', 'C13', 'V56', 'IsMobile_freq', 'V330', 'V85', 'V128', 'V152', 'id_36_freq', 'V11', 'V22', 'uid1_TransactionAmt_mean', 'V304', 'M5_freq', 'V25', 'V190', 'card4', 'V54', 'V109', 'P_emaildomain_1', 'id_13', 'Browser_freq', 'V43', 'V202', 'is_holiday', 'V174', 'V191', 'V219', 'V277', 'id_20', 'V23', 'id_23_freq', 'V142', 'id_09', 'D4_day_min_max', 'V157', 'V146', 'V275', 'Browser', 'V135', 'id_31', 'V263', 'V9', 'P_emaildomain_freq', 'V221', 'uid1_D15_mean', 'C5', 'V288', 'C3', 'V37', 'V187', 'D3_day_min_max', 'V165', 'V296', 'V72', 'V299', 'card3_freq', 'V147', 'D2_day_min_max', 'V272', 'V171', 'V52', 'V38', 'uid2_freq', 'V138', 'V238', 'ProductCD_freq', 'V110', 'D8_day_min_max', 'card6_freq', 'id_15', 'V264', 'C9', 'V322', 'uid1_freq', 'uid2_C13_mean', 'id_13_freq', 'V283', 'V237', 'V159', 'V227', 'V19', 'V62', 'V130', 'id_06', 'V207', 'R_emaildomain_3', 'uid4_D15_std', 'V47', 'V158', 'V282', 'V180', 'V102', 'V154', 'V206', 'V339', 'uid5_unique_amt', 'V229', 'IsMobile', 'uid4_unique_amt', 'V136', 'V100', 'uid1_C13_std', 'V177', 'V66', 'uid1_unique_amt', 'V201', 'V105', 'TransactionAmt', 'V214', 'id_07', 'V257', 'V332', 'V319', 'M4_freq', 'V114', 'C14', 'V134', 'V249', 'id_14_freq', 'V8', 'V51', 'D13_day_min_max', 'card6', 'V315', 'hour_freq', 'V245', 'V61', 'V268', 'V239', 'DeviceInfoMajor_freq', 'DeviceInfoTop_freq', 'V42', 'V148', 'V297', 'V192', 'dayofweek', 'V247', 'V204', 'id_24_freq', 'C11', 'V196', 'uid5_freq', 'V223', 'uid1_C13_mean', 'card2_freq', 'V78', 'V64', 'V295', 'uid1_D15_std', 'V172', 'V333', 'uid4_TransactionAmt_mean', 'V194', 'id_16', 'V280', 'V232', 'uid3_C13_std', 'V189', 'V324', 'V176', 'V321', 'D10_day_min_max', 'V70', 'V306', 'ProductCD', 'card5_freq', 'V60', 'V164', 'V151', 'V44', 'V337', 'V326', 'V13', 'V331', 'V317', 'V30', 'V278', 'OSName', 'id_34_freq', 'id_03', 'card1_freq', 'V7', 'V121', 'uid3_TransactionAmt_mean', 'V71', 'V226', 'M2', 'V302', 'uid3_unique_amt', 'card3', 'V217', 'V150', 'V96', 'V40', 'C2', 'weekofmonth_freq', 'V184', 'C4', 'M5', 'V145', 'V309', 'V173', 'uid5_D15_mean', 'V137', 'V210', 'decimal_value', 'uid5_C13_std', 'V182', 'V87', 'D1_day_min_max', 'V125', 'V252', 'V287', 'V160', 'D15_day_min_max', 'V211', 'V310', 'uid2_C13_std', 'id_26_freq', 'M7_freq', 'V163', 'V312', 'uid1_TransactionAmt_std', 'V131', 'V329', 'R_emaildomain_freq', 'uid2_TransactionAmt_mean', 'V285', 'V274', 'V129', 'V230', 'V74', 'uid4_C13_std', 'V279', 'V3', 'V59', 'dist1', 'uid3_C13_mean', 'V82', 'uid3_TransactionAmt_std', 'V233', 'id_33', 'V144', 'M2_freq', 'V197', 'id_18_freq', 'V153', 'V6', 'V314', 'V97', 'V318', 'V179', 'id_04', 'id_21_freq', 'D5_day_min_max', 'DeviceType', 'V93']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:experiments]",
   "language": "python",
   "name": "conda-env-experiments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
